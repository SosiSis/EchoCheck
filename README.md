# EchoCheck ğŸ›¡ï¸

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://echocheck-w7wuxpftjefzfxhcsrautd.streamlit.app/)

**Your LLM's Harsh (but Fair) Code Reviewer. Preventing AI Hallucinations in Real-Time.**

## ğŸ¯ The Problem

Large Language Models are powerful, but they are notoriously confident, even when they are wrong. In high-stakes environments like coding, legal research, or medical queries, a single hallucination can be catastrophic. Current chatbots don't double-check their work.

## ğŸš€ The Solution

EchoCheck is a **Reflective RAG system** that acts as its own fact-checker before it ever gives you an answer. It retrieves, generates, **critiques**, and then **adapts** - creating a self-improving AI assistant.

## ğŸ—ï¸ Architecture

- **LangGraph**: Orchestrates the reflective workflow
- **Streamlit**: Interactive dashboard with real-time thinking visualization
- **ChromaDB**: Vector database for document storage and retrieval
- **Groq LLaMA**: Primary LLM for generation and critique (fast inference)
- **OpenAI**: Fallback option and embeddings

## ğŸ”„ The Reflection Loop

1. **Initial Retrieval & Generation**: Generate first answer
2. **Self-Critique**: AI critically evaluates its own response
3. **Adaptive Refinement**: If flaws found, improve query and regenerate
4. **Verification**: Deliver verified, high-quality answer

## ğŸ› ï¸ Installation

```bash
pip install -r requirements.txt
```

## ğŸ”§ Setup

1. Create a `.env` file with your API keys:
```
GROQ_API_KEY=your_groq_api_key_here
OPENAI_API_KEY=your_openai_api_key_here  # Optional, for embeddings
USE_GROQ=True
DEFAULT_MODEL=llama3-8b-8192
```

2. Run the application:
```bash
streamlit run app.py
```

## ğŸ“ Project Structure

```
echocheck/
â”œâ”€â”€ app.py                 # Streamlit dashboard
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graph.py          # LangGraph workflow definition
â”‚   â”œâ”€â”€ retriever.py      # Document retrieval logic
â”‚   â”œâ”€â”€ generator.py      # Response generation
â”‚   â”œâ”€â”€ critic.py         # Self-critique system
â”‚   â””â”€â”€ embeddings.py     # Embedding utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py         # Document loading and processing
â”‚   â””â”€â”€ sources/          # Documentation sources
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â””â”€â”€ helpers.py        # Utility functions
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_core.py       # Unit tests
```

## ğŸ¯ Features

- **Real-time Reflection Visualization**: See the AI thinking process
- **Confidence Scoring**: Get reliability metrics for each answer
- **Source Citations**: Hover to see document sources
- **Multi-domain Support**: Currently optimized for coding queries
- **Modular Architecture**: Easy to extend and customize

## ğŸ† Why This Wins

- **Solves Real Problem**: Addresses LLM hallucination concerns
- **Cutting-edge Tech**: Implements Reflective RAG architecture
- **Amazing Demo**: Visual thinking process impresses judges
- **Clear Value**: Makes AI more reliable and trustworthy

## ğŸš€ Demo Scenarios

Try these queries to see the reflection in action:
- "How do I use React's new 'use' hook in a Client Component?"
- "What's the best way to handle state in Next.js 15?"
- "How do I implement streaming with the new OpenAI SDK?"

## ğŸ“ˆ Future Enhancements

- Multi-turn reflection cycles
- HyDE (Hypothetical Document Embeddings) implementation
- Support for additional domains (legal, medical, etc.)
- Advanced confidence scoring algorithms

---

**Built for NSK Hackathon  2025** ğŸ†
